<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge,chrome=1">
<title>Model Mesh Serving: 一种可以大规模部署ML模型的解决方案 - Eclair</title>
<meta name=renderer content="webkit">
<meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1">
<meta http-equiv=cache-control content="no-transform">
<meta http-equiv=cache-control content="no-siteapp">
<meta name=theme-color content="#f8f5ec">
<meta name=msapplication-navbutton-color content="#f8f5ec">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec">
<meta name=author content="LuBingtan"><meta name=description content="What Is Model Mesh Serving 背景: Inference Service in MLOps 作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请"><meta name=keywords content="Hugo,theme,even">
<meta name=generator content="Hugo 0.90.0 with theme even">
<link rel=canonical href=http://localhost:1313/post/modelmesh/>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png>
<link rel=manifest href=/manifest.json>
<link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5>
<link href=/sass/main.min.b5a744db6de49a86cadafb3b70f555ab443f83c307a483402259e94726b045ff.css rel=stylesheet>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous>
<meta property="og:title" content="Model Mesh Serving: 一种可以大规模部署ML模型的解决方案">
<meta property="og:description" content="What Is Model Mesh Serving 背景: Inference Service in MLOps 作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请">
<meta property="og:type" content="article">
<meta property="og:url" content="http://localhost:1313/post/modelmesh/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2021-12-05T00:00:00+08:00">
<meta property="article:modified_time" content="2021-12-05T00:00:00+08:00">
<meta itemprop=name content="Model Mesh Serving: 一种可以大规模部署ML模型的解决方案">
<meta itemprop=description content="What Is Model Mesh Serving 背景: Inference Service in MLOps 作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请"><meta itemprop=datePublished content="2021-12-05T00:00:00+08:00">
<meta itemprop=dateModified content="2021-12-05T00:00:00+08:00">
<meta itemprop=wordCount content="2425">
<meta itemprop=keywords content="machine learning,ai,inference,"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Model Mesh Serving: 一种可以大规模部署ML模型的解决方案">
<meta name=twitter:description content="What Is Model Mesh Serving 背景: Inference Service in MLOps 作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]-->
</head>
<body>
<div id=mobile-navbar class=mobile-navbar>
<div class=mobile-header-logo>
<a href=/ class=logo>Eclair</a>
</div>
<div class=mobile-navbar-icon>
<span></span>
<span></span>
<span></span>
</div>
</div>
<nav id=mobile-menu class="mobile-menu slideout-menu">
<ul class=mobile-menu-list>
<a href=/>
<li class=mobile-menu-item>Home</li>
</a><a href=/post/>
<li class=mobile-menu-item>Archives</li>
</a><a href=/tags/>
<li class=mobile-menu-item>Tags</li>
</a><a href=/categories/>
<li class=mobile-menu-item>Categories</li>
</a>
</ul>
</nav>
<div class=container id=mobile-panel>
<header id=header class=header>
<div class=logo-wrapper>
<a href=/ class=logo>Eclair</a>
</div>
<nav class=site-navbar>
<ul id=menu class=menu>
<li class=menu-item>
<a class=menu-item-link href=/>Home</a>
</li><li class=menu-item>
<a class=menu-item-link href=/post/>Archives</a>
</li><li class=menu-item>
<a class=menu-item-link href=/tags/>Tags</a>
</li><li class=menu-item>
<a class=menu-item-link href=/categories/>Categories</a>
</li>
</ul>
</nav>
</header>
<main id=main class=main>
<div class=content-wrapper>
<div id=content class=content>
<article class=post>
<header class=post-header>
<h1 class=post-title>Model Mesh Serving: 一种可以大规模部署ML模型的解决方案</h1>
<div class=post-meta>
<span class=post-time> 2021-12-05 </span>
<div class=post-category>
<a href=/categories/research/> research </a>
</div>
</div>
</header>
<div class=post-toc id=post-toc>
<h2 class=post-toc-title>Contents</h2>
<div class="post-toc-content always-active">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#what-is-model-mesh-serving>What Is Model Mesh Serving</a>
<ul>
<li><a href=#背景-inference-service-in-mlops>背景: Inference Service in MLOps</a></li>
<li><a href=#ibm-开源的方案-model-mesh-serving>IBM 开源的方案: Model Mesh Serving</a></li>
</ul>
</li>
<li><a href=#how-model-mesh-works>How Model Mesh Works</a>
<ul>
<li><a href=#model-mesh-layer>Model Mesh Layer</a></li>
<li><a href=#model-runtime-adapters>Model Runtime Adapters</a></li>
<li><a href=#model-mesh-seving>Model Mesh Seving</a></li>
</ul>
</li>
<li><a href=#conclusion>Conclusion</a></li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class=post-content>
<h2 id=what-is-model-mesh-serving>What Is Model Mesh Serving</h2>
<h3 id=背景-inference-service-in-mlops>背景: Inference Service in MLOps</h3>
<p>作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请求并返回预测结果.</p>
<p>基于这种需求, <a href=https://github.com/tensorflow/serving>Tensorflow Serving</a>, <a href=https://github.com/pytorch/serve>TorchServe</a>, <a href=https://github.com/triton-inference-server/server>Triton Inference Server</a>, <a href=https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Server_Usage.md>ONNX Runtime Server</a> 等各种 model inference server 开始出现.</p>
<p>不过随着技术以及业务规模的发展, 我们的需求也变得更多, 我们不仅希望把模型部署在服务器上, 更希望能够非常简单的集成<strong>日志/监控/服务发现/负载均衡</strong>等功能, 以及能够非常方便的进行<strong>金丝雀发布/滚动更新</strong>等各种运维操作.</p>
<p>这里有两种思路:</p>
<ul>
<li>
<p>第一种思路就是, 把 model server 当作是一种普通的后端服务:</p>
<p>在这种方案中, AI 工程师需要了解大量和以上功能相关的知识, 或者也可以将模型交付给后端工程师. 然而不管那种方式, 显然都增加了大量的时间以及沟通成本. 并且公司内不同的团队使用不同的框架/接口协议, 这些都会造成额外的接入成本.</p>
</li>
<li>
<p>另一种思路是, 把 model server 当作一种特殊的后端服务:</p>
<p>由 AI 工程师以及后端工程师共同组成一支团队, 建设一个专门用于模型部署的平台. 对于所有模型都以一种标准化的方式进行管理, 并且提供一系列工具对于模型的部署/更新/优化等操作进行封装及简化. 利用这些平台工具, 不同团队的AI工程师都可以非常方便的对于模型进行发布. 这样就可以大大较少模型交付以及迭代的成本.</p>
</li>
</ul>
<p>基于第二种思路, 当前已经出现了一些可用于构建推理平台的开源项目, 例如 <a href=https://github.com/kserve/kserve>kfserving</a> (已经改名为 kserve, 并从 kubeflow 独立出来), <a href=https://github.com/SeldonIO/seldon-core>Seldon Core</a>.</p>
<p>关于这些方案, 具体细节这里就不再赘述了, 感兴趣的同学可以到 github 上进一步了解.</p>
<blockquote>
<p>不过, 笔者认为它们并非是完美的方案, 它们的底层都使用了 trtion, tfserving 等这些 model server, 但是并没有发挥出这些 model server 的全部特性.</p>
</blockquote>
<h3 id=ibm-开源的方案-model-mesh-serving>IBM 开源的方案: Model Mesh Serving</h3>
<p><a href=https://github.com/kserve/modelmesh-serving>Model Mesh Serving</a> 是IBM开源的模型推理方案, 据说已经在内部平稳运行多年了, 现在已经开源, 并作为一个子项目加入了 kserve.</p>
<p>Model Mesh Serving 旨在解决 &lsquo;one model one server&rsquo; 模式 (也是kserve和seldon采用的方案) 的弊端:</p>
<ul>
<li>无法最大限度的有效利用资源</li>
<li>节点上的 Pod 数量是有限的 (100+)</li>
<li>集群的 IP 数量是有限的, 从而导致模型的数量也是有限的</li>
</ul>
<p>Model Mesh Serving 提供了以下一些 feature:</p>
<ul>
<li>
<p>Scalability: 使用 multi-model server, 能够以少量 pod 加载大量模型</p>
</li>
<li>
<p>Cache managerment and HA</p>
<ul>
<li>
<p>管理模型的机制类似于 LRU cache, 动态的加载/卸载模型, 如果某个模型的访问负载很高, 就将它copy到多个 server 实例 (pod)</p>
</li>
<li>
<p>在模型的 copies 之间做 负载均衡/路由转发</p>
</li>
<li>
<p>retrying/rerouting failed requests</p>
</li>
</ul>
</li>
<li>
<p>Intelligent placement and loading</p>
<ul>
<li>加载模型时, 在多个 pod 之间做均衡: 把负载高的模型放到负载小的 pod 中</li>
</ul>
</li>
<li>
<p>Resiliency</p>
<ul>
<li>加载模型失败时, 会在其他 pod 上做重试</li>
</ul>
</li>
<li>
<p>Operational simplicity:</p>
<ul>
<li>模型可以进行滚动更新, 对于 requests 无感</li>
</ul>
</li>
</ul>
<p>Model Mesh Serving 包含的组件</p>
<ul>
<li><code>ServingRuntime</code>: Triton, MLServer, etc</li>
<li><code>ModelMesh</code>: Mesh Layer</li>
<li><code>Runtime Adapters</code>: Adapters to different runtimes</li>
<li><code>ModelMesh Serving</code>: Controller for mesh , runtime , predictor</li>
</ul>
<p><img src=/images/modelmesh/0.2.0-highlevel.png alt=0.2.0-highlevel.png></p>
<p><img src=/images/modelmesh/rt-builtin.png alt=rt-builtin.png></p>
<blockquote>
<p>参考: <a href=https://developer.ibm.com/blogs/kserve-and-watson-modelmesh-extreme-scale-model-inferencing-for-trusted-ai/>ModelMesh and KServe bring eXtreme scale standardized model inferencing on Kubernetes – IBM Developer</a></p>
</blockquote>
<h2 id=how-model-mesh-works>How Model Mesh Works</h2>
<h3 id=model-mesh-layer>Model Mesh Layer</h3>
<p>Model Mesh 的核心就是实现了以下几个 rpc 接口:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-protobuf data-lang=protobuf><span class=kd>service</span> <span class=n>ModelMesh</span> <span class=p>{</span><span class=err>
</span><span class=err></span>  <span class=c1>// Creates a new vmodel id (alias) which maps to a new or existing
</span><span class=c1></span>  <span class=c1>// concrete model, or sets the target model for an existing vmodel
</span><span class=c1></span>  <span class=c1>// to a new or existing concrete model
</span><span class=c1></span>  <span class=k>rpc</span> <span class=n>setVModel</span> <span class=p>(</span><span class=n>SetVModelRequest</span><span class=p>)</span> <span class=k>returns</span> <span class=p>(</span><span class=n>VModelStatusInfo</span><span class=p>)</span> <span class=p>{}</span><span class=err>
</span><span class=err>
</span><span class=err></span>  <span class=c1>// Deletes a vmodel, optionally deleting any referenced concrete
</span><span class=c1></span>  <span class=c1>// models at the same time
</span><span class=c1></span>  <span class=k>rpc</span> <span class=n>deleteVModel</span> <span class=p>(</span><span class=n>DeleteVModelRequest</span><span class=p>)</span> <span class=k>returns</span> <span class=p>(</span><span class=n>DeleteVModelResponse</span><span class=p>)</span> <span class=p>{}</span><span class=err>
</span><span class=err>
</span><span class=err></span>  <span class=c1>// Gets the status of a vmodel, including associated target/active model ids
</span><span class=c1></span>  <span class=c1>// If the vmodel is not found, the returned VModelStatusInfo will have empty
</span><span class=c1></span>  <span class=c1>// active and target model ids and an active model status of NOT_FOUND
</span><span class=c1></span>  <span class=k>rpc</span> <span class=n>getVModelStatus</span> <span class=p>(</span><span class=n>GetVModelStatusRequest</span><span class=p>)</span> <span class=k>returns</span> <span class=p>(</span><span class=n>VModelStatusInfo</span><span class=p>)</span> <span class=p>{}</span><span class=err>
</span><span class=err></span><span class=p>}</span><span class=err>
</span></code></pre></td></tr></table>
</div>
</div><p>下面介绍 modelmesh 中的几个关键概念以及实现.</p>
<h4 id=vmodel-virtual-model-是什么>VModel (virtual model) 是什么?</h4>
<ul>
<li>
<p>可以把 VModel 理解为某一类模型 (例如 人脸识别模型/bert 模型等等), 同时 Model 是具体的某一个模型 (每个具体的模型有不同的参数以及对应的模型文件)</p>
<ul>
<li>model mesh 通过 model id 标记 VModel , 通过 target id 标记 Model. 也就是说 VModel 下所属的每个 Model 都有相同的 model id, 但是有不同的 target id</li>
</ul>
</li>
<li>
<p>model mesh 通过对 VModel + Model 的管理, 实现了模型的发布管理 (有点像 k8s 中的 deployment)</p>
</li>
</ul>
<h4 id=模型是如何被加载的>模型是如何被加载的?</h4>
<p>调用stack:</p>
<ol>
<li>ModelMeshAPI 暴露 setVModel 接口</li>
<li>setVModel 中, 调用了 VModelMananger 的 updateVModel, updateVModel 修改 etcd 上的 Model 记录</li>
<li>ModelMesh watch 到 Model Update Event, 执行 VModelManager 的 processVModel</li>
<li>processVModel 中执行 ensureLoaded → internalOperation → invokeModel →
<ul>
<li>invokeLocalModel → 本地执行 model runtime client
<ul>
<li>model runtime client 是一个 GRPC client, 执行同一个 pod 的 runtime 容器</li>
<li>把请求缓存到一个 <code>loadingQueue</code>中 (loadingQueue 是一个优先队列)</li>
<li>默认异步执行, 如果需要同步执行, 提高优先级, 并等待.</li>
</ul>
</li>
<li>invokeRemote → 执行 remoteClient 或 cacheMissClient (远程 model mesh)
<ul>
<li>runtimeClient 负载均衡策略: 选择很久没用过的节点 (last recently used 最小的)</li>
<li>cacheMissClient 的 lb 策略: 从 prefer 的 instance 中随机选择一个</li>
</ul>
</li>
<li>forwardInvokeModel → 执行 directClient (远程 model mesh)
<ul>
<li>directClient 是一个 Thrift RPC client, 指向上次 (runtimeClient/cacheMissClient) 选择的 model mesh 实例.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>invokeModel 的逻辑 (非常复杂):</p>
<ol>
<li>如果设置了 Local flag, 在本地执行, 如果模型不在本地, 会抛出错误</li>
<li>如果请求有 copy 的 flag, 说明需要复制模型, 加上 unbalanced flag, 递归执行 invokeModel</li>
<li>根据 exclude 参数 (从请求的 context 里获取的), 过滤所有 instance</li>
<li>如果存在可选的 instance, 则根据各种参数判断 需要在本地执行还是远程执行 (比如请求已经被 balaced 了, 则在本地执行)</li>
<li>如果不存在可选的 instance, 说明出现了 cache miss
<ol>
<li>如果来自集群外部的请求, 使用 cacheMissClient 执行.</li>
<li>如果不是, 则在本地执行</li>
</ol>
</li>
</ol>
<h4 id=如何对-inference-请求进行路由转发的>如何对 inference 请求进行路由转发的?</h4>
<ul>
<li>
<p>model mesh 中实现了一个 grpc 代理, 对于带有特定 metadata 的请求进行转发</p>
<blockquote>
<p><a href=https://github.com/kserve/modelmesh-serving/blob/main/docs/predictors/run-inference.md>run-inference.md</a>: you should include an additional metadata parameter <code>mm-balanced = true</code>.</p>
</blockquote>
</li>
<li>
<p>服务启动: NettyServerBuilder → addService → new ServerInterceptor</p>
</li>
<li>
<p>接口调用stack: interceptCall → startCall → ModelMesh.callModel → SidecarModelMesh.callModel → <strong>invokeModel</strong></p>
</li>
</ul>
<h3 id=model-runtime-adapters>Model Runtime Adapters</h3>
<p>Model Runtime Adapters 实现了<code>ModelRuntime</code> 的接口, 并且适配真正的 ModelServer (例如 triton, mlserver, tfserving)</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-protobuf data-lang=protobuf><span class=kd>service</span> <span class=n>ModelRuntime</span> <span class=p>{</span><span class=err>
</span><span class=err></span>  <span class=k>rpc</span> <span class=n>loadModel</span> <span class=p>(</span><span class=n>LoadModelRequest</span><span class=p>)</span> <span class=k>returns</span> <span class=p>(</span><span class=n>LoadModelResponse</span><span class=p>)</span> <span class=p>{}</span><span class=err>
</span><span class=err>
</span><span class=err></span>  <span class=k>rpc</span> <span class=n>unloadModel</span> <span class=p>(</span><span class=n>UnloadModelRequest</span><span class=p>)</span> <span class=k>returns</span> <span class=p>(</span><span class=n>UnloadModelResponse</span><span class=p>)</span> <span class=p>{}</span><span class=err>
</span><span class=err>
</span><span class=err></span>  <span class=c1>// Predict size of not-yet-loaded model - must return almost immediately.
</span><span class=c1></span>  <span class=c1>// Should not perform expensive computation or remote lookups.
</span><span class=c1></span>  <span class=c1>// Should be a conservative estimate.
</span><span class=c1></span>  <span class=k>rpc</span> <span class=n>predictModelSize</span> <span class=p>(</span><span class=n>PredictModelSizeRequest</span><span class=p>)</span> <span class=k>returns</span> <span class=p>(</span><span class=n>PredictModelSizeResponse</span><span class=p>)</span> <span class=p>{}</span><span class=err>
</span><span class=err>
</span><span class=err></span>  <span class=c1>// Calculate size (memory consumption) of currently-loaded model
</span><span class=c1></span>  <span class=k>rpc</span> <span class=n>modelSize</span> <span class=p>(</span><span class=n>ModelSizeRequest</span><span class=p>)</span> <span class=k>returns</span> <span class=p>(</span><span class=n>ModelSizeResponse</span><span class=p>)</span> <span class=p>{}</span><span class=err>
</span><span class=err></span>  <span class=k>rpc</span> <span class=n>runtimeStatus</span> <span class=p>(</span><span class=n>RuntimeStatusRequest</span><span class=p>)</span> <span class=k>returns</span> <span class=p>(</span><span class=n>RuntimeStatusResponse</span><span class=p>)</span> <span class=p>{}</span><span class=err>
</span><span class=err></span><span class=p>}</span><span class=err>
</span></code></pre></td></tr></table>
</div>
</div><p>实现逻辑非常简单, 以 <code>model-mesh-triton-adapter</code>为例:</p>
<p>LoadModel:</p>
<ol>
<li>下载模型(同步)</li>
<li>根据模型类型, 重新设置模型文件名以及路径 (比如 triton 就有些特殊要求, onnx 模型的名字为 model.onnx)</li>
<li>向 model server 发送请求 (triton 的load 接口: RepositoryModelLoad)</li>
<li>返回结果</li>
</ol>
<h3 id=model-mesh-seving>Model Mesh Seving</h3>
<p>Model Mesh Serving 就是 ModelMesh 的controller plane, 它控制了两个 CRD: Predictor 以及 ServingRuntime.</p>
<p>它的功能有:</p>
<ol>
<li>Watch ServiceRuntime, 创建带有 Model Mesh container 的 Deployment</li>
<li>Watch predictor, 访问 Model Mesh 实例, 发送 setVModel/deleteVModel/getVModelStatus 请求</li>
<li>Watch Etcd 中对应 VModel/Model 的 key-value, 转换为 predictor 的 name, 塞进 predictor controller 中处理.</li>
<li>维护 ModelMesh client 以及对应的 grpc resolver</li>
</ol>
<h2 id=conclusion>Conclusion</h2>
<p>感觉 ModelMesh 的想法非常好, 这才是<strong>AI推理服务</strong>的 Serverless !</p>
<p>不过, 感觉这个项目目前还不太成熟.</p>
<ul>
<li>只能支持单个 namespace 的 model mesh, 每扩展一个namespace 就要操作一把, 非常不友好.</li>
<li>需要一个额外的 etcd (在 k8s 之外), 这一点让人感觉非常别扭, 也增加了维护成本.</li>
<li>ModelMesh 作为最重要的组件居然是用 java 写的 (其他都是go), 而且用了一个 IBM 自己的 java 框架 (根本没人用). 可以理解, 但是作为开源项目, 感觉对项目的推广非常不友好.</li>
</ul>
<p>总的来说, 感觉这个项目还是值得一看的. 另外看到matainer也在积极推进项目的发展 (比如说支持多namespace), 希望能早日到达生产可用的状态.</p>
</div>
<div class=post-copyright>
<p class=copyright-item>
<span class=item-title>Author</span>
<span class=item-content>LuBingtan</span>
</p>
<p class=copyright-item>
<span class=item-title>LastMod</span>
<span class=item-content>
2021-12-05
</span>
</p>
<p class=copyright-item>
<span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License target=_blank>Creative Commons Attribution-ShareAlike License</a></span>
</p>
</div>
<footer class=post-footer>
<div class=post-tags>
<a href=/tags/machine-learning/>machine learning</a>
<a href=/tags/ai/>ai</a>
<a href=/tags/inference/>inference</a>
</div>
<nav class=post-nav>
</nav>
</footer>
</article>
</div>
</div>
</main>
<footer id=footer class=footer>
<div class=social-links>
<a href=mailto:your@email.com class="iconfont icon-email" title=email></a>
<a href=http://localhost:1313 class="iconfont icon-stack-overflow" title=stack-overflow></a>
<a href=http://localhost:1313 class="iconfont icon-twitter" title=twitter></a>
<a href=http://localhost:1313 class="iconfont icon-facebook" title=facebook></a>
<a href=http://localhost:1313 class="iconfont icon-linkedin" title=linkedin></a>
<a href=http://localhost:1313 class="iconfont icon-google" title=google></a>
<a href=http://localhost:1313 class="iconfont icon-github" title=github></a>
<a href=http://localhost:1313 class="iconfont icon-weibo" title=weibo></a>
<a href=http://localhost:1313 class="iconfont icon-zhihu" title=zhihu></a>
<a href=http://localhost:1313 class="iconfont icon-douban" title=douban></a>
<a href=http://localhost:1313 class="iconfont icon-pocket" title=pocket></a>
<a href=http://localhost:1313 class="iconfont icon-tumblr" title=tumblr></a>
<a href=http://localhost:1313 class="iconfont icon-instagram" title=instagram></a>
<a href=http://localhost:1313 class="iconfont icon-gitlab" title=gitlab></a>
<a href=http://localhost:1313 class="iconfont icon-bilibili" title=bilibili></a>
<a href=http://localhost:1313/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a>
</div>
<div class=copyright>
<span class=power-by>
Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span>
<span class=division>|</span>
<span class=theme-info>
Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span>
<span class=copyright-year>
&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>lubingtan</span>
</span>
</div>
</footer>
<div class=back-to-top id=back-to-top>
<i class="iconfont icon-up"></i>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script>
</body>
</html>