<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Model Mesh Serving: 一种可以大规模部署ML模型的解决方案 - Eclair</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="LuBingtan" /><meta name="description" content="What Is Model Mesh Serving 背景: Inference Service in MLOps 作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.90.1 with theme even" />


<link rel="canonical" href="http://localhost:1313/post/modelmesh/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.b5a744db6de49a86cadafb3b70f555ab443f83c307a483402259e94726b045ff.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Model Mesh Serving: 一种可以大规模部署ML模型的解决方案" />
<meta property="og:description" content="What Is Model Mesh Serving 背景: Inference Service in MLOps 作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/post/modelmesh/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-12-05T00:00:00+08:00" />
<meta property="article:modified_time" content="2021-12-05T00:00:00+08:00" />

<meta itemprop="name" content="Model Mesh Serving: 一种可以大规模部署ML模型的解决方案">
<meta itemprop="description" content="What Is Model Mesh Serving 背景: Inference Service in MLOps 作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请"><meta itemprop="datePublished" content="2021-12-05T00:00:00+08:00" />
<meta itemprop="dateModified" content="2021-12-05T00:00:00+08:00" />
<meta itemprop="wordCount" content="2425">
<meta itemprop="keywords" content="machine learning,ai,inference," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Model Mesh Serving: 一种可以大规模部署ML模型的解决方案"/>
<meta name="twitter:description" content="What Is Model Mesh Serving 背景: Inference Service in MLOps 作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Eclair</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Eclair</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Model Mesh Serving: 一种可以大规模部署ML模型的解决方案</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-12-05 </span>
        <div class="post-category">
            <a href="/categories/research/"> research </a>
            </div>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#what-is-model-mesh-serving">What Is Model Mesh Serving</a>
          <ul>
            <li><a href="#背景-inference-service-in-mlops">背景: Inference Service in MLOps</a></li>
            <li><a href="#ibm-开源的方案-model-mesh-serving">IBM 开源的方案: Model Mesh Serving</a></li>
          </ul>
        </li>
        <li><a href="#how-model-mesh-works">How Model Mesh Works</a>
          <ul>
            <li><a href="#model-mesh-layer">Model Mesh Layer</a></li>
            <li><a href="#model-runtime-adapters">Model Runtime Adapters</a></li>
            <li><a href="#model-mesh-seving">Model Mesh Seving</a></li>
          </ul>
        </li>
        <li><a href="#conclusion">Conclusion</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h2 id="what-is-model-mesh-serving">What Is Model Mesh Serving</h2>
<h3 id="背景-inference-service-in-mlops">背景: Inference Service in MLOps</h3>
<p>作为一个算法工程师/Data scientist, 往往希望在训练结束后, 能够快速的把模型部署在服务器中, 这样就能让模型很方便的接收外部请求并返回预测结果.</p>
<p>基于这种需求, <a href="https://github.com/tensorflow/serving">Tensorflow Serving</a>, <a href="https://github.com/pytorch/serve">TorchServe</a>, <a href="https://github.com/triton-inference-server/server">Triton Inference Server</a>, <a href="https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Server_Usage.md">ONNX Runtime Server</a> 等各种 model inference server 开始出现.</p>
<p>不过随着技术以及业务规模的发展, 我们的需求也变得更多, 我们不仅希望把模型部署在服务器上, 更希望能够非常简单的集成<strong>日志/监控/服务发现/负载均衡</strong>等功能, 以及能够非常方便的进行<strong>金丝雀发布/滚动更新</strong>等各种运维操作.</p>
<p>这里有两种思路:</p>
<ul>
<li>
<p>第一种思路就是, 把 model server 当作是一种普通的后端服务:</p>
<p>在这种方案中, AI 工程师需要了解大量和以上功能相关的知识, 或者也可以将模型交付给后端工程师. 然而不管那种方式, 显然都增加了大量的时间以及沟通成本. 并且公司内不同的团队使用不同的框架/接口协议, 这些都会造成额外的接入成本.</p>
</li>
<li>
<p>另一种思路是, 把 model server 当作一种特殊的后端服务:</p>
<p>由 AI 工程师以及后端工程师共同组成一支团队,  建设一个专门用于模型部署的平台. 对于所有模型都以一种标准化的方式进行管理, 并且提供一系列工具对于模型的部署/更新/优化等操作进行封装及简化. 利用这些平台工具, 不同团队的AI工程师都可以非常方便的对于模型进行发布. 这样就可以大大较少模型交付以及迭代的成本.</p>
</li>
</ul>
<p>基于第二种思路, 当前已经出现了一些可用于构建推理平台的开源项目, 例如 <a href="https://github.com/kserve/kserve">kfserving</a> (已经改名为 kserve, 并从 kubeflow 独立出来), <a href="https://github.com/SeldonIO/seldon-core">Seldon Core</a>.</p>
<p>关于这些方案, 具体细节这里就不再赘述了, 感兴趣的同学可以到 github 上进一步了解.</p>
<blockquote>
<p>不过, 笔者认为它们并非是完美的方案, 它们的底层都使用了 trtion, tfserving 等这些 model server, 但是并没有发挥出这些 model server 的全部特性.</p>
</blockquote>
<h3 id="ibm-开源的方案-model-mesh-serving">IBM 开源的方案: Model Mesh Serving</h3>
<p><a href="https://github.com/kserve/modelmesh-serving">Model Mesh Serving</a> 是IBM开源的模型推理方案, 据说已经在内部平稳运行多年了, 现在已经开源, 并作为一个子项目加入了 kserve.</p>
<p>Model Mesh Serving  旨在解决 &lsquo;one model one server&rsquo; 模式 (也是kserve和seldon采用的方案) 的弊端:</p>
<ul>
<li>无法最大限度的有效利用资源</li>
<li>节点上的 Pod 数量是有限的 (100+)</li>
<li>集群的 IP 数量是有限的, 从而导致模型的数量也是有限的</li>
</ul>
<p>Model Mesh Serving  提供了以下一些 feature:</p>
<ul>
<li>
<p>Scalability: 使用 multi-model server, 能够以少量 pod 加载大量模型</p>
</li>
<li>
<p>Cache managerment and HA</p>
<ul>
<li>
<p>管理模型的机制类似于 LRU cache, 动态的加载/卸载模型, 如果某个模型的访问负载很高, 就将它copy到多个 server 实例 (pod)</p>
</li>
<li>
<p>在模型的 copies 之间做 负载均衡/路由转发</p>
</li>
<li>
<p>retrying/rerouting failed requests</p>
</li>
</ul>
</li>
<li>
<p>Intelligent placement and loading</p>
<ul>
<li>加载模型时, 在多个 pod 之间做均衡: 把负载高的模型放到负载小的 pod 中</li>
</ul>
</li>
<li>
<p>Resiliency</p>
<ul>
<li>加载模型失败时, 会在其他 pod 上做重试</li>
</ul>
</li>
<li>
<p>Operational simplicity:</p>
<ul>
<li>模型可以进行滚动更新, 对于 requests 无感</li>
</ul>
</li>
</ul>
<p>Model Mesh Serving 包含的组件</p>
<ul>
<li><code>ServingRuntime</code>: Triton, MLServer, etc</li>
<li><code>ModelMesh</code>: Mesh Layer</li>
<li><code>Runtime Adapters</code>: Adapters to different runtimes</li>
<li><code>ModelMesh Serving</code>: Controller for mesh , runtime , predictor</li>
</ul>
<p><img src="/images/modelmesh/0.2.0-highlevel.png" alt="0.2.0-highlevel.png"></p>
<p><img src="/images/modelmesh/rt-builtin.png" alt="rt-builtin.png"></p>
<blockquote>
<p>参考: <a href="https://developer.ibm.com/blogs/kserve-and-watson-modelmesh-extreme-scale-model-inferencing-for-trusted-ai/">ModelMesh and KServe bring eXtreme scale standardized model inferencing on Kubernetes – IBM Developer</a></p>
</blockquote>
<h2 id="how-model-mesh-works">How Model Mesh Works</h2>
<h3 id="model-mesh-layer">Model Mesh Layer</h3>
<p>Model Mesh 的核心就是实现了以下几个 rpc 接口:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-protobuf" data-lang="protobuf"><span class="kd">service</span> <span class="n">ModelMesh</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Creates a new vmodel id (alias) which maps to a new or existing
</span><span class="c1"></span>  <span class="c1">// concrete model, or sets the target model for an existing vmodel
</span><span class="c1"></span>  <span class="c1">// to a new or existing concrete model
</span><span class="c1"></span>  <span class="k">rpc</span> <span class="n">setVModel</span> <span class="p">(</span><span class="n">SetVModelRequest</span><span class="p">)</span> <span class="k">returns</span> <span class="p">(</span><span class="n">VModelStatusInfo</span><span class="p">)</span> <span class="p">{}</span><span class="err">
</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Deletes a vmodel, optionally deleting any referenced concrete
</span><span class="c1"></span>  <span class="c1">// models at the same time
</span><span class="c1"></span>  <span class="k">rpc</span> <span class="n">deleteVModel</span> <span class="p">(</span><span class="n">DeleteVModelRequest</span><span class="p">)</span> <span class="k">returns</span> <span class="p">(</span><span class="n">DeleteVModelResponse</span><span class="p">)</span> <span class="p">{}</span><span class="err">
</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Gets the status of a vmodel, including associated target/active model ids
</span><span class="c1"></span>  <span class="c1">// If the vmodel is not found, the returned VModelStatusInfo will have empty
</span><span class="c1"></span>  <span class="c1">// active and target model ids and an active model status of NOT_FOUND
</span><span class="c1"></span>  <span class="k">rpc</span> <span class="n">getVModelStatus</span> <span class="p">(</span><span class="n">GetVModelStatusRequest</span><span class="p">)</span> <span class="k">returns</span> <span class="p">(</span><span class="n">VModelStatusInfo</span><span class="p">)</span> <span class="p">{}</span><span class="err">
</span><span class="err"></span><span class="p">}</span><span class="err">
</span></code></pre></td></tr></table>
</div>
</div><p>下面介绍 modelmesh 中的几个关键概念以及实现.</p>
<h4 id="vmodel-virtual-model-是什么">VModel (virtual model) 是什么?</h4>
<ul>
<li>
<p>可以把 VModel 理解为某一类模型 (例如 人脸识别模型/bert 模型等等), 同时 Model 是具体的某一个模型 (每个具体的模型有不同的参数以及对应的模型文件)</p>
<ul>
<li>model mesh 通过 model id 标记 VModel , 通过 target id 标记 Model. 也就是说 VModel 下所属的每个 Model 都有相同的 model id, 但是有不同的 target id</li>
</ul>
</li>
<li>
<p>model mesh 通过对 VModel + Model 的管理, 实现了模型的发布管理 (有点像 k8s 中的 deployment)</p>
</li>
</ul>
<h4 id="模型是如何被加载的">模型是如何被加载的?</h4>
<p>调用stack:</p>
<ol>
<li>ModelMeshAPI 暴露 setVModel 接口</li>
<li>setVModel 中, 调用了 VModelMananger 的 updateVModel, updateVModel 修改 etcd 上的 Model 记录</li>
<li>ModelMesh watch 到 Model Update Event, 执行 VModelManager 的 processVModel</li>
<li>processVModel 中执行 ensureLoaded →  internalOperation → invokeModel →
<ul>
<li>invokeLocalModel → 本地执行 model runtime client
<ul>
<li>model runtime client 是一个 GRPC client, 执行同一个 pod 的 runtime 容器</li>
<li>把请求缓存到一个 <code>loadingQueue</code>中 (loadingQueue 是一个优先队列)</li>
<li>默认异步执行, 如果需要同步执行, 提高优先级, 并等待.</li>
</ul>
</li>
<li>invokeRemote → 执行  remoteClient 或 cacheMissClient (远程 model mesh)
<ul>
<li>runtimeClient  负载均衡策略:  选择很久没用过的节点 (last recently used 最小的)</li>
<li>cacheMissClient 的 lb 策略: 从 prefer 的 instance 中随机选择一个</li>
</ul>
</li>
<li>forwardInvokeModel → 执行 directClient (远程 model mesh)
<ul>
<li>directClient 是一个 Thrift RPC client, 指向上次 (runtimeClient/cacheMissClient) 选择的 model mesh 实例.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>invokeModel 的逻辑 (非常复杂):</p>
<ol>
<li>如果设置了 Local  flag, 在本地执行, 如果模型不在本地, 会抛出错误</li>
<li>如果请求有 copy 的 flag, 说明需要复制模型, 加上 unbalanced flag, 递归执行 invokeModel</li>
<li>根据 exclude 参数 (从请求的 context 里获取的), 过滤所有 instance</li>
<li>如果存在可选的 instance, 则根据各种参数判断 需要在本地执行还是远程执行 (比如请求已经被 balaced 了, 则在本地执行)</li>
<li>如果不存在可选的 instance, 说明出现了 cache miss
<ol>
<li>如果来自集群外部的请求, 使用  cacheMissClient  执行.</li>
<li>如果不是, 则在本地执行</li>
</ol>
</li>
</ol>
<h4 id="如何对-inference-请求进行路由转发的">如何对 inference 请求进行路由转发的?</h4>
<ul>
<li>
<p>model mesh 中实现了一个 grpc 代理, 对于带有特定 metadata 的请求进行转发</p>
<blockquote>
<p><a href="https://github.com/kserve/modelmesh-serving/blob/main/docs/predictors/run-inference.md">run-inference.md</a>: you should include an additional metadata parameter <code>mm-balanced = true</code>.</p>
</blockquote>
</li>
<li>
<p>服务启动: NettyServerBuilder → addService → new ServerInterceptor</p>
</li>
<li>
<p>接口调用stack: interceptCall →  startCall → ModelMesh.callModel → SidecarModelMesh.callModel → <strong>invokeModel</strong></p>
</li>
</ul>
<h3 id="model-runtime-adapters">Model Runtime Adapters</h3>
<p>Model Runtime Adapters 实现了<code>ModelRuntime</code> 的接口, 并且适配真正的 ModelServer (例如 triton, mlserver, tfserving)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-protobuf" data-lang="protobuf"><span class="kd">service</span> <span class="n">ModelRuntime</span> <span class="p">{</span><span class="err">
</span><span class="err"></span>  <span class="k">rpc</span> <span class="n">loadModel</span> <span class="p">(</span><span class="n">LoadModelRequest</span><span class="p">)</span> <span class="k">returns</span> <span class="p">(</span><span class="n">LoadModelResponse</span><span class="p">)</span> <span class="p">{}</span><span class="err">
</span><span class="err">
</span><span class="err"></span>  <span class="k">rpc</span> <span class="n">unloadModel</span> <span class="p">(</span><span class="n">UnloadModelRequest</span><span class="p">)</span> <span class="k">returns</span> <span class="p">(</span><span class="n">UnloadModelResponse</span><span class="p">)</span> <span class="p">{}</span><span class="err">
</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Predict size of not-yet-loaded model - must return almost immediately.
</span><span class="c1"></span>  <span class="c1">// Should not perform expensive computation or remote lookups.
</span><span class="c1"></span>  <span class="c1">// Should be a conservative estimate.
</span><span class="c1"></span>  <span class="k">rpc</span> <span class="n">predictModelSize</span> <span class="p">(</span><span class="n">PredictModelSizeRequest</span><span class="p">)</span> <span class="k">returns</span> <span class="p">(</span><span class="n">PredictModelSizeResponse</span><span class="p">)</span> <span class="p">{}</span><span class="err">
</span><span class="err">
</span><span class="err"></span>  <span class="c1">// Calculate size (memory consumption) of currently-loaded model
</span><span class="c1"></span>  <span class="k">rpc</span> <span class="n">modelSize</span> <span class="p">(</span><span class="n">ModelSizeRequest</span><span class="p">)</span> <span class="k">returns</span> <span class="p">(</span><span class="n">ModelSizeResponse</span><span class="p">)</span> <span class="p">{}</span><span class="err">
</span><span class="err"></span>  <span class="k">rpc</span> <span class="n">runtimeStatus</span> <span class="p">(</span><span class="n">RuntimeStatusRequest</span><span class="p">)</span> <span class="k">returns</span> <span class="p">(</span><span class="n">RuntimeStatusResponse</span><span class="p">)</span> <span class="p">{}</span><span class="err">
</span><span class="err"></span><span class="p">}</span><span class="err">
</span></code></pre></td></tr></table>
</div>
</div><p>实现逻辑非常简单, 以  <code>model-mesh-triton-adapter</code>为例:</p>
<p>LoadModel:</p>
<ol>
<li>下载模型(同步)</li>
<li>根据模型类型, 重新设置模型文件名以及路径 (比如 triton 就有些特殊要求, onnx 模型的名字为 model.onnx)</li>
<li>向 model server 发送请求 (triton 的load 接口: RepositoryModelLoad)</li>
<li>返回结果</li>
</ol>
<h3 id="model-mesh-seving">Model Mesh Seving</h3>
<p>Model Mesh Serving 就是 ModelMesh 的controller plane, 它控制了两个 CRD: Predictor 以及 ServingRuntime.</p>
<p>它的功能有:</p>
<ol>
<li>Watch ServiceRuntime, 创建带有 Model Mesh container 的 Deployment</li>
<li>Watch predictor, 访问 Model Mesh 实例, 发送 setVModel/deleteVModel/getVModelStatus 请求</li>
<li>Watch Etcd 中对应 VModel/Model 的 key-value, 转换为 predictor 的 name, 塞进 predictor controller 中处理.</li>
<li>维护 ModelMesh client 以及对应的 grpc resolver</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>感觉 ModelMesh 的想法非常好, 这才是<strong>AI推理服务</strong>的 Serverless !</p>
<p>不过, 感觉这个项目目前还不太成熟.</p>
<ul>
<li>只能支持单个 namespace 的 model mesh, 每扩展一个namespace 就要操作一把, 非常不友好.</li>
<li>需要一个额外的 etcd (在 k8s 之外), 这一点让人感觉非常别扭, 也增加了维护成本.</li>
<li>ModelMesh 作为最重要的组件居然是用 java 写的 (其他都是go), 而且用了一个 IBM 自己的 java 框架 (根本没人用). 可以理解, 但是作为开源项目, 感觉对项目的推广非常不友好.</li>
</ul>
<p>总的来说, 感觉这个项目还是值得一看的. 另外看到matainer也在积极推进项目的发展 (比如说支持多namespace), 希望能早日到达生产可用的状态.</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">LuBingtan</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2021-12-05
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" target="_blank">Creative Commons Attribution-ShareAlike License</a></span>
  </p>
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/machine-learning/">machine learning</a>
          <a href="/tags/ai/">ai</a>
          <a href="/tags/inference/">inference</a>
          </div>
      <nav class="post-nav">
        
        
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="http://localhost:1313" class="iconfont icon-twitter" title="twitter"></a>
      <a href="http://localhost:1313" class="iconfont icon-facebook" title="facebook"></a>
      <a href="http://localhost:1313" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="http://localhost:1313" class="iconfont icon-google" title="google"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://localhost:1313" class="iconfont icon-douban" title="douban"></a>
      <a href="http://localhost:1313" class="iconfont icon-pocket" title="pocket"></a>
      <a href="http://localhost:1313" class="iconfont icon-tumblr" title="tumblr"></a>
      <a href="http://localhost:1313" class="iconfont icon-instagram" title="instagram"></a>
      <a href="http://localhost:1313" class="iconfont icon-gitlab" title="gitlab"></a>
      <a href="http://localhost:1313" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="http://localhost:1313/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2021<span class="heart"><i class="iconfont icon-heart"></i></span><span>lubingtan</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>








</body>
</html>
